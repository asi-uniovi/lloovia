# vim: syntax=python tabstop=4 expandtab
# coding: utf-8

rule synthetic_workload:
    input: synthetic_LTWP, synthetic_STWP, synthetic_example

rule copy_pregenerated_synthetic_workload:
    input: "data/paper/traces_synthetic_{case}_{prediction}.csv"
    output: "data/processed/traces_synthetic_{case}_{prediction}.csv"
    message: "Copying pre-generated synthetic {wildcards.prediction} for case {wildcards.case}"
    shell: "cp {input} {output}"

rule copy_synthetic_example_load:
    input: paper_example
    output: synthetic_example
    message: "Copying pre-generated synthetic example"
    shell: "cp {input} {output}"

rule generate_synthetic_workload:
    output: "data/processed/traces_synthetic_{case}_{prediction}.csv"
    message: "Generating new sythetic {wildcards.prediction} for case {wildcards.case}"
    run:
        import src.data.make_dataset as m
        m.generate_case(wildcards.case, LEVELS,
                        extension="_" + wildcards.prediction, folder="data/processed")


##############################################################################
# WIKIPEDIA DATA
##############################################################################

rule wikipedia_traces:
    input: processed_wikipedia

rule copy_already_processed_wikipedia_traces:
    output: processed_wikipedia
    input: paper_wikipedia
    message: "Copying preprocessed wikipedia traces"
    shell: "cp {input} {output}"


# Recompute wikipedia traces
# Definitions
wikipedia_years = ["%d"%y for y in range(2010, 2016)]
wikipedia_months = ["%02d"%(m+1) for m in range(12)]
raw_wikipedia_traces = expand("/tmp/logs-{year}/{year}-{month}.tbz", year=wikipedia_years, month=wikipedia_months)
processed_wikipedia_years = expand("data/interim/traces_en_wikipedia_{year}.csv", year=wikipedia_years)
def tbz_in_a_year(w):
    """Returns the list of files which compose the traces for a given year"""
    year=w.year
    return ["/tmp/logs-{}/{}-{:02}.tbz".format(year, year, month+1) for month in range(12)]

# Rules
rule download_wikipedia_traces:
    input: raw_wikipedia_traces

rule download_wikipedia_months:
    output: "/tmp/logs-{year}/{year}-{month}.tbz"
    message: "Downloading wikipedia traces for {wildcards.year}-{wildcards.month}"
    run:
        year, month = int(wildcards.year), int(wildcards.month)
        import src.data.wikipedia_download as wd
        w = wd.WikipediaLogDownloader("/tmp/logs-{}".format(year))
        w.download_month(year, month)

rule process_wikipedia_year:
    input: tbz_in_a_year
    output: "data/interim/traces_en_wikipedia_{year}.csv"
    message: "Processing wikipedia data for year {wildcards.year}"
    run:
        import src.process_data.process_wikipedia_traces as wp
        w = wp.WikipediaAnalyzer()
        w.analyze("/tmp/logs-{}".format(wildcards.year))
        (w.getdf()
         .query("project == 'en'")
         .views
         .to_csv("{}".format(output), sep=";")
         )

rule join_wikipedia_years:
    input: processed_wikipedia_years
    output: "data/processed/traces_en_wikipedia.csv"
    shell: "cat {input} > {output}"


##############################################################################
# Cloud providers data
##############################################################################

rule providers_data:
    input: processed_providers

rule copy_preprocessed_provider_data:
    input: "data/paper/providers_{provider}_data.csv"
    output: "data/processed/providers_{provider}_data.csv"
    message: "Copying provider data for {wildcards.provider}"
    shell: "cp {input} {output}"

rule process_amazon_data:
    input: "/tmp/raw_amazon_data.csv.gz"
    output: "data/processed/providers_amazon_data.csv"
    run:
        import src.process_data.amazon_data as ad
        ad.simplify_amazon_data(str(input), str(output))

rule download_amazon_data:
    output: "/tmp/raw_amazon_data.csv.gz"
    run:
        import src.data.amazon_donwload as ad
        ad.download_amazon_data(str(output))

rule download_azure_data:
    output: "/tmp/azure_web_page.html"
    shell: "wget -O {output} https://azure.microsoft.com/en-us/pricing/details/virtual-machines/linux"

rule scrape_azure_data:
    input: "/tmp/azure_web_page.html"
    output: "data/interim/azure_full_data.json"
    run:
        import src.data.azure_scrap as az
        az.AzurePrices(str(input)).scrape_and_save(str(output))
      
#################################################################################
# Benchmark results
#################################################################################

rule copy_benchmark_results:
    input: "data/paper/benchmarks_{benchmark}.csv"
    output: "data/processed/benchmarks_{benchmark}.csv"
    message: "Copying benchmark results for {wildcards.benchmark}"
    shell: "cp {input} {output}"


vm_used = dict(
    amazon=["c4.2xlarge", "c4.xlarge", "m4.2xlarge", "m4.large", "m4.xlarge"],
    azure=["A5", "A6", "Basic_A0", "Basic_A1", "Basic_A2", "Basic_A3", "ExtraSmall",
           "Large", "Medium", "Small", "Standard_D11_v2", "Standard_D12_v2", 
           "Standard_D1_v2", "Standard_D2_v2", "Standard_D3_v2"]
        )

rule summarize_oldisim:
    input:  expand("data/interim/qps_{provider}.json", provider = PROVIDERS)
    output: "data/processed/benchmarks_oldisim.csv"
    message: "Summarizing oldisim benchmark results"
    run:
        import src.process_data.process_oldisim_results as por
        inputs = []
        for provider in PROVIDERS:
            inputs.append(dict(name=provider.capitalize(),
                               filename="data/interim/qps_%s.json" % provider,
                               vm_types=vm_used[provider]
                                )
                         )
        por.generate_rph_csv(inputs, str(output))

def oldisim_raw_files_for_provider(w):
    import os
    folder = "data/raw/oldisim/%s" % w.provider
    return [os.path.join(folder, f) for f in os.listdir(folder)]

rule process_oldisim:
    input: oldisim_raw_files_for_provider
    output: "data/interim/qps_{provider}.json"
    message: "Processing oldisim data for {wildcards.provider}"
    run:
        import src.process_data.process_oldisim_results as por
        import os
        por.generate_qps_json(os.path.dirname(str(input[0])), str(output))

  
